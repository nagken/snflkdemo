"""
Embedding Generation Module for Snowflake Cortex GenAI Pipeline
Generates vector embeddings using Snowflake Cortex functions.
"""

import logging
import time
from typing import List, Dict, Any, Optional
import pandas as pd
from snowflake.snowpark import Session
from snowflake.snowpark.functions import col, lit, when
from snowflake.snowpark.types import StructType, StructField, StringType, ArrayType, DoubleType

from utils import get_session, load_settings, setup_logging, calculate_token_cost
from telemetry_task import TelemetryTracker

logger = setup_logging()


class EmbeddingGenerator:
    """Handles vector embedding generation using Snowflake Cortex"""
    
    def __init__(self, session: Optional[Session] = None):
        """Initialize with Snowpark session and configuration"""
        self.session = session or get_session()
        self.settings = load_settings()
        self.embedding_model = self.settings.get('cortex', {}).get('embedding', {}).get('model', 'text-embedding-ada-002')
        self.batch_size = self.settings.get('cortex', {}).get('embedding', {}).get('batch_size', 100)
        self.max_tokens = self.settings.get('cortex', {}).get('embedding', {}).get('max_tokens', 8192)
        
        # Table names from settings
        self.raw_table = self.settings.get('data', {}).get('tables', {}).get('raw', 'media_raw')
        self.embeddings_table = self.settings.get('data', {}).get('tables', {}).get('embeddings', 'media_embeddings')
        
        # Initialize telemetry tracker
        self.telemetry = TelemetryTracker(session=self.session)
        
    def setup_embeddings_table(self):
        """Create embeddings table with vector column"""
        try:
            create_sql = f"""
            CREATE OR REPLACE TABLE {self.embeddings_table} (
                doc_id STRING PRIMARY KEY,
                filename STRING,
                content STRING,
                content_chunk STRING,
                chunk_index NUMBER DEFAULT 0,
                embedding_model STRING,
                embedding ARRAY,
                token_count NUMBER,
                chunk_size NUMBER,
                embedding_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),
                metadata VARIANT,
                FOREIGN KEY (doc_id) REFERENCES {self.raw_table}(doc_id)
            )
            COMMENT = 'Document embeddings generated by Snowflake Cortex'
            """
            
            self.session.sql(create_sql).collect()
            logger.info(f"Created embeddings table: {self.embeddings_table}")
            
            # Create vector index for similarity search
            index_sql = f"""
            CREATE OR REPLACE VECTOR INDEX IF NOT EXISTS vec_idx_{self.embeddings_table}
            ON {self.embeddings_table}(embedding)
            """
            
            try:
                self.session.sql(index_sql).collect()
                logger.info(f"Created vector index on {self.embeddings_table}")
            except Exception as e:
                logger.warning(f"Vector index creation failed (may not be supported): {e}")
                
        except Exception as e:
            logger.error(f"Failed to setup embeddings table: {e}")
            raise
    
    def chunk_text(self, text: str, chunk_size: int = 1000, overlap: int = 100) -> List[str]:
        """Split text into overlapping chunks for embedding"""
        if not text or len(text.strip()) == 0:
            return []
        
        words = text.split()
        chunks = []
        
        # Estimate words per chunk (rough approximation: 1 token ≈ 0.75 words)
        words_per_chunk = int(chunk_size * 0.75)
        overlap_words = int(overlap * 0.75)
        
        for i in range(0, len(words), words_per_chunk - overlap_words):
            chunk_words = words[i:i + words_per_chunk]
            if chunk_words:  # Only add non-empty chunks
                chunks.append(' '.join(chunk_words))
        
        return chunks if chunks else [text]  # Return original text if chunking fails
    
    def estimate_token_count(self, text: str) -> int:
        """Estimate token count for text (rough approximation)"""
        # Rough estimation: 1 token ≈ 4 characters for English text
        return max(1, len(text) // 4)
    
    def generate_single_embedding(self, text: str, doc_id: str) -> Dict[str, Any]:
        """Generate embedding for a single text chunk"""
        start_time = time.time()
        
        try:
            # Truncate text if too long
            if len(text) > self.max_tokens * 4:  # Rough char to token conversion
                text = text[:self.max_tokens * 4]
                logger.warning(f"Truncated text for {doc_id} to fit token limit")
            
            # Generate embedding using Cortex
            embedding_sql = f"""
            SELECT SNOWFLAKE.CORTEX.EMBED_TEXT_768('{self.embedding_model}', '{text}') as embedding
            """
            
            # Escape single quotes in text
            escaped_text = text.replace("'", "''")
            embedding_sql = f"""
            SELECT SNOWFLAKE.CORTEX.EMBED_TEXT_768('{self.embedding_model}', '{escaped_text}') as embedding
            """
            
            result = self.session.sql(embedding_sql).collect()
            
            if result and result[0]['EMBEDDING']:
                latency_ms = (time.time() - start_time) * 1000
                token_count = self.estimate_token_count(text)
                cost = calculate_token_cost(token_count, self.embedding_model)
                
                # Log telemetry
                self.telemetry.log_operation(
                    operation_type='embedding_generation',
                    model_name=self.embedding_model,
                    input_tokens=token_count,
                    output_tokens=0,  # Embeddings don't have output tokens
                    latency_ms=latency_ms,
                    cost_usd=cost,
                    success_flag=True,
                    query_text=text[:100] + "..." if len(text) > 100 else text,
                    metadata={'doc_id': doc_id, 'text_length': len(text)}
                )
                
                return {
                    'embedding': result[0]['EMBEDDING'],
                    'token_count': token_count,
                    'latency_ms': latency_ms,
                    'cost_usd': cost,
                    'success': True
                }
            else:
                raise Exception("No embedding returned from Cortex")
                
        except Exception as e:
            latency_ms = (time.time() - start_time) * 1000
            error_msg = str(e)
            
            # Log failed telemetry
            self.telemetry.log_operation(
                operation_type='embedding_generation',
                model_name=self.embedding_model,
                input_tokens=self.estimate_token_count(text),
                output_tokens=0,
                latency_ms=latency_ms,
                cost_usd=0.0,
                success_flag=False,
                error_message=error_msg,
                query_text=text[:100] + "..." if len(text) > 100 else text,
                metadata={'doc_id': doc_id}
            )
            
            logger.error(f"Error generating embedding for {doc_id}: {error_msg}")
            return {
                'embedding': None,
                'error': error_msg,
                'latency_ms': latency_ms,
                'success': False
            }
    
    def process_document(self, doc_id: str, force_reprocess: bool = False) -> bool:
        """Process a single document to generate embeddings"""
        try:
            # Check if document already has embeddings
            if not force_reprocess:
                check_sql = f"SELECT COUNT(*) as cnt FROM {self.embeddings_table} WHERE doc_id = '{doc_id}'"
                result = self.session.sql(check_sql).collect()
                if result[0]['CNT'] > 0:
                    logger.info(f"Document {doc_id} already has embeddings, skipping...")
                    return True
            
            # Get document content
            content_sql = f"""
            SELECT doc_id, filename, content 
            FROM {self.raw_table} 
            WHERE doc_id = '{doc_id}'
            """
            
            docs = self.session.sql(content_sql).collect()
            
            if not docs:
                logger.error(f"Document {doc_id} not found in {self.raw_table}")
                return False
            
            doc = docs[0]
            content = doc['CONTENT']
            filename = doc['FILENAME']
            
            if not content or len(content.strip()) == 0:
                logger.warning(f"Document {doc_id} has no content")
                return False
            
            # Chunk the document
            chunks = self.chunk_text(content)
            logger.info(f"Processing {doc_id}: {len(chunks)} chunks")
            
            successful_chunks = 0
            
            # Process each chunk
            for chunk_idx, chunk in enumerate(chunks):
                if not chunk.strip():
                    continue
                
                # Generate embedding
                result = self.generate_single_embedding(chunk, f"{doc_id}_chunk_{chunk_idx}")
                
                if result['success']:
                    # Insert embedding into table
                    insert_data = [{
                        'doc_id': doc_id,
                        'filename': filename,
                        'content': content,
                        'content_chunk': chunk,
                        'chunk_index': chunk_idx,
                        'embedding_model': self.embedding_model,
                        'embedding': result['embedding'],
                        'token_count': result['token_count'],
                        'chunk_size': len(chunk),
                        'metadata': {
                            'processing_latency_ms': result['latency_ms'],
                            'processing_cost_usd': result['cost_usd'],
                            'chunk_method': 'overlap_chunking'
                        }
                    }]
                    
                    df = self.session.create_dataframe(insert_data)
                    df.write.mode("append").save_as_table(self.embeddings_table)
                    successful_chunks += 1
                    
                else:
                    logger.error(f"Failed to process chunk {chunk_idx} for {doc_id}: {result.get('error')}")
            
            success_rate = successful_chunks / len(chunks) if chunks else 0
            logger.info(f"Processed {doc_id}: {successful_chunks}/{len(chunks)} chunks successful ({success_rate:.1%})")
            
            return successful_chunks > 0
            
        except Exception as e:
            logger.error(f"Error processing document {doc_id}: {e}")
            return False
    
    def process_all_documents(self, force_reprocess: bool = False) -> Dict[str, Any]:
        """Process all documents in the raw table"""
        try:
            # Get list of documents to process
            if force_reprocess:
                docs_sql = f"SELECT DISTINCT doc_id FROM {self.raw_table}"
            else:
                docs_sql = f"""
                SELECT DISTINCT r.doc_id 
                FROM {self.raw_table} r
                LEFT JOIN {self.embeddings_table} e ON r.doc_id = e.doc_id
                WHERE e.doc_id IS NULL
                """
            
            docs = self.session.sql(docs_sql).collect()
            
            if not docs:
                logger.info("No documents to process")
                return {'processed_count': 0, 'success_count': 0, 'failed_count': 0}
            
            results = {
                'processed_count': len(docs),
                'success_count': 0,
                'failed_count': 0,
                'processed_docs': [],
                'failed_docs': []
            }
            
            logger.info(f"Processing {len(docs)} documents...")
            
            for doc_row in docs:
                doc_id = doc_row['DOC_ID']
                
                try:
                    if self.process_document(doc_id, force_reprocess):
                        results['success_count'] += 1
                        results['processed_docs'].append(doc_id)
                    else:
                        results['failed_count'] += 1
                        results['failed_docs'].append(doc_id)
                        
                except Exception as e:
                    logger.error(f"Error processing {doc_id}: {e}")
                    results['failed_count'] += 1
                    results['failed_docs'].append(doc_id)
            
            success_rate = results['success_count'] / results['processed_count'] if results['processed_count'] > 0 else 0
            logger.info(f"Batch processing complete: {results['success_count']}/{results['processed_count']} successful ({success_rate:.1%})")
            
            return results
            
        except Exception as e:
            logger.error(f"Error in batch processing: {e}")
            raise
    
    def get_embedding_stats(self) -> Dict[str, Any]:
        """Get statistics about generated embeddings"""
        try:
            stats_sql = f"""
            SELECT 
                COUNT(*) as total_embeddings,
                COUNT(DISTINCT doc_id) as unique_documents,
                COUNT(DISTINCT embedding_model) as unique_models,
                AVG(token_count) as avg_tokens_per_chunk,
                SUM(token_count) as total_tokens,
                AVG(chunk_size) as avg_chunk_size,
                MIN(embedding_timestamp) as first_embedding,
                MAX(embedding_timestamp) as latest_embedding
            FROM {self.embeddings_table}
            """
            
            result = self.session.sql(stats_sql).collect()[0]
            
            return {
                'total_embeddings': result['TOTAL_EMBEDDINGS'],
                'unique_documents': result['UNIQUE_DOCUMENTS'],
                'unique_models': result['UNIQUE_MODELS'],
                'avg_tokens_per_chunk': round(result['AVG_TOKENS_PER_CHUNK'] or 0, 2),
                'total_tokens': result['TOTAL_TOKENS'],
                'avg_chunk_size': round(result['AVG_CHUNK_SIZE'] or 0, 2),
                'first_embedding': result['FIRST_EMBEDDING'],
                'latest_embedding': result['LATEST_EMBEDDING']
            }
            
        except Exception as e:
            logger.error(f"Error getting embedding stats: {e}")
            return {}
    
    def test_embedding_quality(self, test_query: str = "artificial intelligence automation") -> List[Dict[str, Any]]:
        """Test embedding quality with a sample query"""
        try:
            # Generate embedding for test query
            query_result = self.generate_single_embedding(test_query, "test_query")
            
            if not query_result['success']:
                logger.error(f"Failed to generate query embedding: {query_result.get('error')}")
                return []
            
            query_embedding = query_result['embedding']
            
            # Find similar documents using vector similarity
            similarity_sql = f"""
            SELECT 
                doc_id,
                filename,
                content_chunk,
                VECTOR_COSINE_SIMILARITY(embedding, PARSE_JSON('{query_embedding}')::ARRAY) as similarity_score,
                token_count,
                chunk_index
            FROM {self.embeddings_table}
            ORDER BY similarity_score DESC
            LIMIT 5
            """
            
            results = self.session.sql(similarity_sql).collect()
            
            similar_docs = []
            for row in results:
                similar_docs.append({
                    'doc_id': row['DOC_ID'],
                    'filename': row['FILENAME'],
                    'content_preview': row['CONTENT_CHUNK'][:200] + "..." if len(row['CONTENT_CHUNK']) > 200 else row['CONTENT_CHUNK'],
                    'similarity_score': float(row['SIMILARITY_SCORE']),
                    'token_count': row['TOKEN_COUNT'],
                    'chunk_index': row['CHUNK_INDEX']
                })
            
            logger.info(f"Found {len(similar_docs)} similar documents for query: '{test_query}'")
            return similar_docs
            
        except Exception as e:
            logger.error(f"Error testing embedding quality: {e}")
            return []


def main():
    """Main function for testing and CLI usage"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Snowflake Cortex Embedding Generator')
    parser.add_argument('--setup', action='store_true', help='Setup embeddings table')
    parser.add_argument('--process-all', action='store_true', help='Process all documents')
    parser.add_argument('--process-doc', type=str, help='Process specific document by ID')
    parser.add_argument('--force', action='store_true', help='Force reprocessing')
    parser.add_argument('--stats', action='store_true', help='Show embedding statistics')
    parser.add_argument('--test', type=str, help='Test embedding quality with query')
    
    args = parser.parse_args()
    
    try:
        generator = EmbeddingGenerator()
        
        if args.setup:
            print("Setting up embeddings table...")
            generator.setup_embeddings_table()
            print("Embeddings table setup complete!")
        
        if args.process_all:
            print("Processing all documents...")
            results = generator.process_all_documents(force_reprocess=args.force)
            print(f"Processed {results['success_count']}/{results['processed_count']} documents successfully")
        
        if args.process_doc:
            print(f"Processing document: {args.process_doc}")
            if generator.process_document(args.process_doc, force_reprocess=args.force):
                print("Document processed successfully!")
            else:
                print("Failed to process document!")
        
        if args.stats:
            print("Embedding Statistics:")
            stats = generator.get_embedding_stats()
            for key, value in stats.items():
                print(f"  {key}: {value}")
        
        if args.test:
            print(f"Testing embedding quality with query: '{args.test}'")
            similar_docs = generator.test_embedding_quality(args.test)
            for i, doc in enumerate(similar_docs, 1):
                print(f"\n{i}. Document: {doc['doc_id']} (similarity: {doc['similarity_score']:.3f})")
                print(f"   Preview: {doc['content_preview']}")
    
    except KeyboardInterrupt:
        print("\nOperation cancelled by user")
    except Exception as e:
        logger.error(f"Error in main: {e}")
        print(f"Error: {e}")


if __name__ == "__main__":
    main()